{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T20:25:58.899222Z",
     "start_time": "2025-05-27T20:25:57.966309Z"
    }
   },
   "source": [
    "from tqdm.auto import tqdm\n",
    "import datasets"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T20:25:59.109991Z",
     "start_time": "2025-05-27T20:25:59.107104Z"
    }
   },
   "source": [
    "# Supress progress bars which appear every time a task is downloaded\n",
    "datasets.utils.logging.set_verbosity_error()"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T20:26:01.460076Z",
     "start_time": "2025-05-27T20:25:59.222750Z"
    }
   },
   "source": [
    "dataset = datasets.load_dataset(\"lexglue\", \"unfair_tos\")\n",
    "dataset[\"train\"].to_pandas()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                    answer index  \\\n",
       "0                    Other     0   \n",
       "1              Arbitration     1   \n",
       "2        Contract by using     2   \n",
       "3        Unilateral change     3   \n",
       "4   Unilateral termination     4   \n",
       "5  Limitation of liability     5   \n",
       "6            Choice of law     6   \n",
       "7             Jurisdiction     7   \n",
       "8          Content removal     8   \n",
       "\n",
       "                                                text  \n",
       "0                 last updated date : may 15 , 2017   \n",
       "1  arbitration notice : unless you opt out of arb...  \n",
       "2  you acknowledge and agree that , by accessing ...  \n",
       "3  academia.edu reserves the right , at its sole ...  \n",
       "4  academia.edu reserves the right to suspend or ...  \n",
       "5  neither academia.edu nor any other person or e...  \n",
       "6  these terms and any action related thereto wil...  \n",
       "7  the exclusive jurisdiction and venue of any ip...  \n",
       "8  amazon reserves the right ( but not the obliga...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Other</td>\n",
       "      <td>0</td>\n",
       "      <td>last updated date : may 15 , 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Arbitration</td>\n",
       "      <td>1</td>\n",
       "      <td>arbitration notice : unless you opt out of arb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Contract by using</td>\n",
       "      <td>2</td>\n",
       "      <td>you acknowledge and agree that , by accessing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Unilateral change</td>\n",
       "      <td>3</td>\n",
       "      <td>academia.edu reserves the right , at its sole ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Unilateral termination</td>\n",
       "      <td>4</td>\n",
       "      <td>academia.edu reserves the right to suspend or ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Limitation of liability</td>\n",
       "      <td>5</td>\n",
       "      <td>neither academia.edu nor any other person or e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Choice of law</td>\n",
       "      <td>6</td>\n",
       "      <td>these terms and any action related thereto wil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Jurisdiction</td>\n",
       "      <td>7</td>\n",
       "      <td>the exclusive jurisdiction and venue of any ip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Content removal</td>\n",
       "      <td>8</td>\n",
       "      <td>amazon reserves the right ( but not the obliga...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-05-27T20:26:01.672531Z"
    }
   },
   "source": [
    "import openai\n",
    "import datasets\n",
    "from tqdm.auto import tqdm\n",
    "from legalbench.utils import generate_prompts\n",
    "from legalbench.evaluation import evaluate\n",
    "\n",
    "# --- 1. Configuración del Cliente OpenAI/Ollama y LLM ---\n",
    "GENERATIVE_MODEL = \"qwen3:8b\" # O el modelo que estés usando en Ollama\n",
    "TASK_NAME = \"unfair_tos\"\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    base_url='http://localhost:11434/v1',\n",
    "    api_key='ollama', # Ollama usa \"ollama\" o cualquier string si no se requiere auth\n",
    ")\n",
    "\n",
    "# --- 2. Cargar Datos de la Tarea y Plantilla de Prompt ---\n",
    "# Cargar el conjunto de datos para la tarea específica desde Hugging Face\n",
    "print(f\"Cargando dataset para la tarea: {TASK_NAME}...\")\n",
    "dataset = datasets.load_dataset(\"lex_glue\", TASK_NAME)\n",
    "test_df = dataset[\"test\"].to_pandas()\n",
    "test_df = test_df[:100]\n",
    "# train_df = dataset[\"train\"].to_pandas() # Para few-shot si los quieres añadir al prompt base\n",
    "\n",
    "# Cargar la plantilla de prompt base para la tarea\n",
    "# Asegúrate de que la ruta 'tasks/TASK_NAME/base_prompt.txt' sea correcta\n",
    "# relativa a donde ejecutas tu script/notebook.\n",
    "prompt_template_path = f\"legalbench/tasks/{TASK_NAME}/base_prompt.txt\"\n",
    "print(f\"Cargando plantilla de prompt desde: {prompt_template_path}...\")\n",
    "try:\n",
    "    with open(prompt_template_path) as f:\n",
    "        prompt_template = f.read()\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: No se encontró el archivo de prompt en {prompt_template_path}\")\n",
    "    print(\"Asegúrate de que la carpeta 'tasks' esté en el mismo directorio que tu script/notebook,\")\n",
    "    print(\"o ajusta la ruta según sea necesario.\")\n",
    "    exit()\n",
    "\n",
    "# --- 3. Generar Prompts para los Datos de Prueba ---\n",
    "# La función generate_prompts llenará las plantillas con los datos de test_df\n",
    "print(\"Generando prompts para el conjunto de prueba...\")\n",
    "prompts_for_llm = generate_prompts(prompt_template=prompt_template, data_df=test_df)\n",
    "print(f\"Se generaron {len(prompts_for_llm)} prompts.\")\n",
    "#print(prompts_for_llm[0])\n",
    "if not prompts_for_llm:\n",
    "    print(\"No se generaron prompts. Revisa tu plantilla y datos.\")\n",
    "    exit()\n",
    "\n",
    "# Mostrar un ejemplo del prompt generado (opcional)\n",
    "# print(\"\\nEjemplo de prompt generado:\")\n",
    "# print(prompts_for_llm[0])\n",
    "\n",
    "# --- 4. Obtener Generaciones (Predicciones) del LLM ---\n",
    "llm_generations = []\n",
    "print(f\"\\nObteniendo predicciones del LLM ({GENERATIVE_MODEL}) para {len(prompts_for_llm)} instancias...\")\n",
    "\n",
    "# Es buena idea definir un system prompt general si tu modelo responde mejor con uno.\n",
    "# Para LegalBench, los prompts suelen ser autocontenidos con instrucciones y ejemplos.\n",
    "# El base_prompt.txt de abercrombie ya tiene ejemplos\n",
    "# por lo que un system prompt vacío o muy genérico podría ser suficiente.\n",
    "SYSTEM_PROMPT = \"\"\"You are a clause classification assistant. \"\n",
    "    \"Given a clause, classify it into one of the following categories: \"\n",
    "    \"Arbitration, Unilateral change, Content removal, Jurisdiction, Choice of law, \"\n",
    "    \"Limitation of liability, Unilateral termination, Contract by using, Other. \"\n",
    "    \"Respond with ONLY the category name and nothing else.\"\"\"\n",
    "# O puedes dejarlo vacío si el prompt base es suficiente:\n",
    "# SYSTEM_PROMPT = \"\"\n",
    "\n",
    "#prompts_for_llm = prompts_for_llm[:15]\n",
    "\n",
    "for user_prompt_content in tqdm(prompts_for_llm):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_content}\n",
    "    ]\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=GENERATIVE_MODEL,\n",
    "            messages=messages,\n",
    "            temperature=0.0  # Generalmente bueno para tareas de clasificación/extracción\n",
    "        )\n",
    "        generated_text = response.choices[0].message.content.strip()\n",
    "        # NUEVO: Añadir paso de post-procesamiento\n",
    "        cleaned_generation = \"\"\n",
    "        possible_labels = [\n",
    "            \"Arbitration\", \"Unilateral change\", \"Content removal\", \"Jurisdiction\",\n",
    "            \"Choice of law\", \"Limitation of liability\", \"Unilateral termination\",\n",
    "            \"Contract by using\", \"Other\"\n",
    "        ]\n",
    "\n",
    "        # Intenta extraer la etiqueta si el LLM la incluye con \"Label: \"\n",
    "        if \"label:\" in generated_text.lower():\n",
    "            parts = generated_text.lower().split(\"label:\")\n",
    "            potential_label = parts[-1].strip()\n",
    "            # Ahora verifica si esta etiqueta potencial es una de las conocidas\n",
    "            # (esto ayuda si el LLM añade texto extra DESPUÉS de la etiqueta)\n",
    "            for pl in possible_labels:\n",
    "                if pl.lower() == potential_label: # Comparación exacta inicial\n",
    "                    cleaned_generation = pl\n",
    "                    break\n",
    "                # Si no hay coincidencia exacta, intenta ver si la etiqueta es una subcadena\n",
    "                # (por si el LLM dice \"Label: Arbitration clause\" en lugar de solo \"Arbitration\")\n",
    "                if not cleaned_generation and pl.lower() in potential_label:\n",
    "                     cleaned_generation = pl # Toma la primera que coincida como subcadena\n",
    "                     # Podrías querer lógica más sofisticada aquí si hay ambigüedad\n",
    "\n",
    "            # Si después de \"Label:\" no se encontró una etiqueta válida,\n",
    "            # se podría intentar una búsqueda más general en todo el texto.\n",
    "            if not cleaned_generation:\n",
    "                # Fallback: buscar la última aparición de alguna etiqueta conocida en el texto\n",
    "                # Esto es menos preciso y puede dar falsos positivos\n",
    "                best_match = \"\"\n",
    "                best_pos = -1\n",
    "                for known_label in possible_labels:\n",
    "                    pos = generated_text.lower().rfind(known_label.lower())\n",
    "                    if pos > best_pos: # Encuentra la última aparición\n",
    "                        best_pos = pos\n",
    "                        best_match = known_label\n",
    "                cleaned_generation = best_match\n",
    "\n",
    "        else:\n",
    "            # Si \"Label:\" no está, intenta encontrar la última etiqueta conocida mencionada\n",
    "            best_match = \"\"\n",
    "            best_pos = -1\n",
    "            for known_label in possible_labels:\n",
    "                # Buscamos la etiqueta completa para evitar coincidencias parciales no deseadas\n",
    "                # (ej. \"law\" en \"Choice of law\" vs \"contract by using law\")\n",
    "                # Usamos expresiones regulares para buscar palabras completas (case-insensitive)\n",
    "                import re\n",
    "                if re.search(r'\\b' + re.escape(known_label.lower()) + r'\\b', generated_text.lower()):\n",
    "                    # Esta lógica es simple; si múltiples etiquetas están, puede no ser la correcta.\n",
    "                    # La estrategia de abajo de buscar la última es una heurística.\n",
    "                    pos = generated_text.lower().rfind(known_label.lower())\n",
    "                    if pos > best_pos:\n",
    "                        best_pos = pos\n",
    "                        best_match = known_label\n",
    "            cleaned_generation = best_match\n",
    "\n",
    "\n",
    "        if not cleaned_generation: # Si aún no se pudo extraer\n",
    "             print(f\"WARN: No se pudo extraer una etiqueta válida de: '{generated_text[:100]}...' Se usará la salida original normalizada.\")\n",
    "             # En este caso, la normalización de evaluate() intentará limpiarla, pero probablemente falle.\n",
    "             # O podrías asignar un placeholder como \"extracción fallida\"\n",
    "             llm_generations.append(generated_text) # Usar el texto original para ver qué hace normalize()\n",
    "        else:\n",
    "             llm_generations.append(cleaned_generation)\n",
    "    except Exception as e:\n",
    "        print(f\"Error al llamar al API de Ollama: {e}\")\n",
    "        # Decide cómo manejar errores: añadir un placeholder, reintentar, o parar.\n",
    "        # Por ahora, añadiremos un string vacío para no romper la evaluación.\n",
    "        llm_generations.append(\"\") # O un valor que sepas que será incorrecto\n",
    "\n",
    "if not llm_generations or len(llm_generations) != len(prompts_for_llm):\n",
    "    print(\"Error: No se pudieron obtener todas las generaciones del LLM.\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- 5. Evaluar las Generaciones ---\n",
    "ground_truth_answers = test_df[\"answer\"].tolist()\n",
    "\n",
    "print(\"\\nEvaluando las predicciones...\")\n",
    "# La función evaluate tomará el nombre de la tarea, las predicciones de tu LLM,\n",
    "# y las respuestas correctas.\n",
    "score = evaluate(TASK_NAME, llm_generations, ground_truth_answers)\n",
    "\n",
    "print(f\"\\nResultado de la evaluación para la tarea '{TASK_NAME}' con el modelo '{GENERATIVE_MODEL}':\")\n",
    "print(f\"Score: {score}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando dataset para la tarea: unfair_tos...\n",
      "Cargando plantilla de prompt desde: legalbench/tasks/unfair_tos/base_prompt.txt...\n",
      "Generando prompts para el conjunto de prueba...\n",
      "Se generaron 100 prompts.\n",
      "\n",
      "Obteniendo predicciones del LLM (qwen3:8b) para 100 instancias...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f422e39fb80e4444ba27a7358b8688ef"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
